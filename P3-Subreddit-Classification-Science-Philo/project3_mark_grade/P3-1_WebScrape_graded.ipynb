{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52dfc452-50db-429d-aa3f-d584a2f7551a",
   "metadata": {},
   "source": [
    "# GA Project 3: Classification of Subreddits\n",
    "This project is a classification machine learning model with a single goal - to be able to distinguish and accurately identify which subreddit each of the entries come from. The workflow is loosely as follows:\n",
    "\n",
    "1. **[Webscraping](P3-1_WebScrape.ipynb)** using the [pushshift api](https://github.com/pushshift/api) to obtain the necessary submissions from each subreddit;\n",
    "2. Performing **[preprocessing and EDA](P3-2_PP_EDA.ipynb)** on the data obtained, and;\n",
    "3. Evaluating a plethora of **relevant [models](P3-3_Model_Consolidated.ipynb)** with **hyperparameter tuning** to figure out which is the best predictive model for this job, and evaluate its results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1dcff1-0e99-406e-b620-db21146331a6",
   "metadata": {},
   "source": [
    "## 0 Background of Scenario\n",
    "*\"For roughly 98 percent of the last 2,500 years of Western intellectual history, **philosophy was considered the mother of all knowledge**. It generated most of the fields of research still with us today. This is why we continue to call our highest degrees Ph.D.’s, namely, philosophy doctorates. At the same time, we live an age in which many seem no longer sure what philosophy is or is good for anymore. Most seem to see it as a highly abstracted discipline with little if any bearing on objective reality — something more akin to art, literature or religion. All have plenty to say about reality. But the overarching assumption is that none of it actually qualifies as knowledge until proven scientifically.\"*\n",
    "\n",
    "This is the introduction from a [2012 nytimes opinion post](https://archive.nytimes.com/opinionator.blogs.nytimes.com/2012/04/05/philosophy-is-not-a-science/) about Philosophy, and how it has gone from being mankind's source of knowledge and wisdom to being relegated to the realm of arts. In its place, 'Science' has become our society's fundamental source of objective truth, trusting more in scientific methods than the lofty abstractions philosophers offer. \n",
    "\n",
    "One observation you may have, however, is that the language used in these two fields often share some commonality. Both are still treated as formal disciplines, complete with their own insular communities of scholars, debating using evidence-based and research-backed arguments. The common man would such naturally have trouble distinguishing between the two, and may struggle to separate scientific fact from philosophical reasoning, if only given the text.\n",
    "\n",
    "The moderators of r/science and r/philosophy have caught on to this issue, and teamed up to try and solve the problem using **machine learning** to tell difference between the rhetorics. \n",
    "\n",
    "Armed with an army of data from each subreddit, they hope to create a tool that can help their redditers to figure out (*with reasonable accuracy*) **whether their articles/posts/thoughts are better suited for scientific discussion or philosophical debate, and more importantly which subreddit it should go into**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90855e80-947b-4dc4-88c8-52485b7a6388",
   "metadata": {},
   "source": [
    "><font color=red>Clear explanation and problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382fb05-c6cb-4d8d-bb87-28b5f5e1c79c",
   "metadata": {},
   "source": [
    "## 1 Data Extraction - Scraping from Subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d457bb7-8075-430d-9d05-28ab42a4d753",
   "metadata": {},
   "source": [
    "This notebook aims to document and run the webscraping algorithm used to generate the submission data from r/science and r/philosophy, utilising the [pushshift api](https://github.com/pushshift/api). This is done with the following considerations:\n",
    "\n",
    "1. Importing 25,000 submissions (posts) from each subreddit;\n",
    "2. Webscraping about 200 submissions each time, with a 3-5 second time delay;\n",
    "3. Compile the submissions into a single dataframe with the outputs 'subreddit', 'title', 'selftext', 'url', and 'author'. Note that for the purposes of this analysis, only the title and selftext are considered for data, and the subreddit as the target vector;\n",
    "4. Standardise post scraping to before Monday, 3 October 2022 16:00:00 GMT (*Epoch Time: 1664812800*), so that the data is consistent each time the scraping function is run.\n",
    "\n",
    "For generalisation, the function was defined with variable inputs, in case there is a need to rerun again with different inputs or targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2183f1b7-f0ac-4d49-bd87-0ad073fd8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For Web Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# For Pre-Processing/Word Cleaning\n",
    "import re, nltk, string\n",
    "import demoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Miscellaneous\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050b305-671d-4392-b6ac-de39a01a455f",
   "metadata": {},
   "source": [
    "><font color=red>Import only libraries used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac55d7c4-231b-4633-a8ba-2625c49e099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a switch so that we can run through the code without running the scrape\n",
    "run_scrape = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd14d587-e154-4f10-8fee-bd2ab2a2219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting base API call url\n",
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823a0644-85a5-417a-b5d2-5add8f401e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to scrape subreddits\n",
    "def get_subm(posts_per_subred, scrape_size, before, subred_list, columns):\n",
    "    \n",
    "    # Setting up loop function to run\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Parameter variable setting\n",
    "    posts_required = posts_per_subred\n",
    "    size = scrape_size\n",
    "    posts_before = before\n",
    "    subreddits = subred_list\n",
    "\n",
    "    # Setting empty dataframe to start concatenation from\n",
    "    data_df = pd.DataFrame(columns = columns)\n",
    "\n",
    "    # Iterating through the two subreddits\n",
    "    for subreddit in subreddits:\n",
    "\n",
    "        # Set the starting time to sieve posts from designated epoch time\n",
    "        time_before = posts_before\n",
    "\n",
    "        # Iterating through enough scrapes to make up required data size\n",
    "        # for iterations in range(0,int(posts_required/size)):\n",
    "        while len(data_df[data_df['subreddit'] == subreddit].index) < posts_required:\n",
    "\n",
    "            # Parameters for API\n",
    "            # Note: size is based on min(set_size, total required - total scraped)\n",
    "            # This means the last scrape would get only the balance remaining (which may be <200)\n",
    "            # This is necessary because sometimes the webscraper does not scrape the full 200, only 199\n",
    "            # I suspect this is due to some complication on the reddit end of the api\n",
    "            # By scraping this way, we will always get our desired amount of data, as long as it available\n",
    "            params = {\n",
    "                'subreddit': subreddit, # subreddit to scrape from\n",
    "                # Number of scrapes per iteration\n",
    "                'size': min(size,int(posts_required - len(data_df[data_df['subreddit'] == subreddit].index))), \n",
    "                'before': time_before, # time ceiling to where to start scraping from\n",
    "            }\n",
    "\n",
    "            # Assign the data from the scraped text to a temporary dataframe\n",
    "            rd_data_tmp = requests.get(url, params)\n",
    "            data_df_tmp = pd.DataFrame(rd_data_tmp.json()['data'])[columns]\n",
    "\n",
    "            # Set the scrape timing for next iteration to just before last scraped post\n",
    "            time_before = rd_data_tmp.json()['data'][-1]['created_utc']\n",
    "\n",
    "            # Add data to master dataframe\n",
    "            data_df = pd.concat([data_df,data_df_tmp]).sort_index(ignore_index = True)\n",
    "\n",
    "            # Time delay between scrapes, and milestone tracking\n",
    "            # Elapsed time\n",
    "            time_curr = time.time()\n",
    "            hours = int((time_curr - start_time)/(60 ** 2))\n",
    "            minutes = int(round(((time_curr - start_time)/60) % 60,0))\n",
    "            seconds = int(round((time_curr - start_time) % 60,0))\n",
    "\n",
    "            # Print Elapsed time and data volumne\n",
    "            print(f'''Elapsed time(h.m.s):{str(hours)}.{str(minutes)}.{str(seconds)\n",
    "                    }, Curr data vol: {len(data_df.index)}''')\n",
    "\n",
    "            # Loop will pause for a random period between 3 & 5s\n",
    "            time.sleep(np.random.choice(range(3,6))) \n",
    "\n",
    "    for i in subreddits:\n",
    "        print(f\"data volume ({i}): {len(data_df[data_df['subreddit'] == i].index)} entries\")\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d0fd0d-a5d7-4de3-a3c7-ed259f228ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Actual data scrape step to get data\n",
    "\n",
    "if run_scrape == True:\n",
    "    data_df = get_subm(posts_per_subred = 25000, # 25000 per subreddit\n",
    "                       scrape_size = 200, # scraping 200 per scrape\n",
    "                       before = 1664812800, # Epoch time for Monday, 3 October 2022 16:00:00 GMT\n",
    "                       subred_list = ['science','philosophy'], # Required subreddits\n",
    "                       columns = ['subreddit','author','selftext','title','url']) # Required fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e0c5d5-4299-4b66-9d56-0ae5702c24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a csv for later use\n",
    "if run_scrape == True:\n",
    "    data_df.to_csv('Datasets/reddit_data.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb260bce-323e-45aa-8234-1376edf77781",
   "metadata": {},
   "source": [
    "><font color=red>Comprehensive use of function and explanation, well done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a54da-acf0-483d-bc3b-fe2fc94db1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
